検索拡張生成 (RAG) は、大規模な言語モデルの出力を最適化するプロセスです。そのため、応答を生成する前に、トレーニングデータソース以外の信頼できる知識ベースを参照します。大規模言語モデル (LLM) は、膨大な量のデータに基づいてトレーニングされ、何十億ものパラメーターを使用して、質問への回答、言語の翻訳、文章の完成などのタスクのためのオリジナルの出力を生成します。RAG は、LLM の既に強力な機能を、モデルを再トレーニングすることなく、特定の分野や組織の内部ナレッジベースに拡張します。LLM のアウトプットを改善するための費用対効果の高いアプローチであるため、さまざまな状況で関連性、正確性、有用性を維持できます。

LLM は、インテリジェントチャットボットや他の自然言語処理 (NLP) アプリケーションを支える重要な AI テクノロジーです。目標は、信頼できるナレッジソースを相互参照することで、さまざまなコンテキストでユーザーの質問に答えることができるボットを作成することです。残念なことに、LLM テクノロジーの性質上、LLM の応答には予測がつかないという問題があります。さらに、LLM のトレーニングデータは静的であり、蓄積された知識に期限を設けています。

LLM の既知の課題には以下が含まれます:

答えがないのに虚偽の情報を提示すること。
ユーザーが特定の最新の応答を期待している場合に、古くなった、または一般的な情報を提示する。
権限のないソースからの回答の作成。
用語の混乱により、異なるトレーニングソースが同じ用語を使用して異なる内容について話すため、回答が不正確になります。
大規模言語モデルは、最新の出来事について常に情報を得ることを拒否しながら、常に自信を持ってすべての質問に答える、熱心すぎる新入社員と考えることができます。残念ながら、このような態度はユーザーの信頼に悪影響を及ぼす可能性があり、チャットボットに真似してほしくないものです!

RAG は、これらの課題のいくつかを解決するための 1 つのアプローチです。LLM をリダイレクトして、信頼できる事前に決められたナレッジソースから関連情報を取得します。組織は生成されたテキスト出力をより細かく制御でき、ユーザーは LLM がどのように応答を生成するかについての洞察を得ることができます。

セマンティック検索は、LLM アプリケーションに膨大な外部ナレッジソースを追加したい組織の RAG 結果を強化します。現代の企業は、マニュアル、よくある質問、調査レポート、カスタマーサービスに関するガイド、人事に関する書類など、膨大な情報をさまざまなシステムに保存しています。コンテキスト検索は大規模では難しく、結果として生成出力の品質が低下します。

セマンティック検索の技術は、さまざまな情報の大規模なデータベースをスキャンし、データをより正確に取得できます。たとえば、「昨年、機械の修理にいくら費やされたか?」などの質問に答えることができ、質問を関連文書にマッピングして、検索結果の代わりに特定のテキストを返すというものです。その後、開発者はその回答を使用して LLM により多くのコンテキストを提供できます。

RAG の従来の検索の解決やキーワード検索の解決では、知識集約型のタスクでは結果が限られます。また、開発者は手作業でデータを準備する際に、単語の埋め込み、文書のチャンク化、その他の複雑な作業にも対処する必要があります。これとは対照的に、セマンティック検索技術はナレッジベースの準備作業をすべて行うため、開発者はそのような作業を行う必要はありません。また、RAG ペイロードの品質を最大限に高めるために、意味的に関連性の高いパッセージとトークンワードを関連性の高い順に生成します。

RAGとは、Retrieval Augmented Generationの略で、自社に蓄積された大量の業務文書・規定などの社内情報、外部の最新情報を活用する手段として、信頼できるデータを検索して情報を抽出し、それに基づいて大規模言語モデル（LLM）に回答させる方法のことです。日本語では、検索拡張生成と言います。

　生成AIは人間と同じように自然な会話を行うことが可能で、さらに要約や翻訳などさまざまなタスクを実行できる汎用性を持っています。しかし、残念ながら生成AIは常に完璧な回答をしてくれるわけではありません。生成AIの知能にあたるLLMは、学習済みのデータを基に、質問内容（プロンプト）から統計的に確率が高い回答を生成します。したがって、学習済みのデータに含まれない情報に基づいては回答できません。例えば、汎用的なLLMに対し、自社の社内規定や今日の株価について質問しても、LLM単独では正しく回答することはできません。
　一方、生成AIのビジネス活用が進展する中で、自社に蓄積された社内情報や外部の最新情報を活用することへのニーズは高まっています。汎用的なタスクだけでなく、自社の業務に特化したタスクを生成AIに任せたいと考えた場合、こうしたニーズに直面するでしょう。RAGは、こうしたニーズを満たすために有効なアプローチです。RAGを用いることで、生成AIが社内情報などに基づいた回答を行うことができるようになります。また、生成AIが事実に基づかない回答を生成し、もっともらしい嘘をつく“ハルシネーション”を低減させることにもつながります。

RAGとは
Retrieval-Augmented Generation (RAG) は、大規模言語モデル（LLM）によるテキスト生成に、外部情報の検索を組み合わせることで、回答精度を向上させる技術のこと。

「検索拡張生成」、「取得拡張生成」などと訳されます。外部情報の検索を組み合わせることで、大規模言語モデル（LLM）の出力結果を簡単に最新の情報に更新できるようになる効果や、出力結果の根拠が明確になり、事実に基づかない情報を生成する現象（ハルシネーション）を抑制する効果などが期待されています。
（読み：ラグ）

RAGの構成
RAGには大きく2段階のプロセスが存在します。

プロセス①：検索フェーズ
ユーザから入力された質問に関連する情報を、データベースや文書から検索します。

プロセス②：生成フェーズ
①で得られた検索結果とユーザからの質問を組み合わせたプロンプトを大規模言語モデルに入力し、テキスト生成結果を取得します。

また、RAGの回答精度向上には、プロセス①の検索フェーズの工夫が特に重要になります。外部情報のフォーマットをどのように整備するか、検索方式（キーワード検索やベクトル検索など）をどのように設計するかなどを考慮する必要があります。

RAGの強み
RAGの特徴は、一般的な大規模言語モデルに加えて、情報検索を組み合わせていることです。この構成により、下記のような強みが発揮されます。

・情報更新の容易性
RAGでは外部情報とされる文書やデータベース内の情報を最新化することで、最新の情報を即座に大規模言語モデルから出力結果に反映させることができます。大規模言語モデルのみ用いた構成の場合に比べて、モデルの再学習が必要でなくなるため、コストが抑制される可能性があります。

・出力結果の信頼性
RAGでは外部情報の検索結果を利用して回答を出力するため、根拠が明らかになり、回答の信頼性が高まります。また、大規模言語モデルによるハルシネーションのリスク軽減も期待できます。

RAGのユースケース
RAGを活用することで、社内に蓄積されたあらゆる業務分野の文章ナレッジを効率的に活用することができます。
例えば、顧客からの問合せに対応するサポートデスクでは、よくある質問（FAQ）やマニュアルを外部情報とし、チャットボットやオペレーター支援ツールをRAGの方式で開発することにより、従来オペレーターが問合せを受けてから社内マニュアルや過去のFAQを手作業で探していた時間を短縮することが可能です。また、業務習熟度に関わらずオペレーターの応答品質を一定値まで高めることが期待でき、ひいては顧客の問合せ待ち時間の短縮に繋がります。それ以外でも、リアルタイムで商品・在庫の情報が更新される金融や小売業界や、医療や法務など、根拠が明らかで正確な回答が求められるシーンでの大規模言語モデルに利用に向けて、RAGの活用が進んでいます。

　企業の生成AI（人工知能）活用が進むにつれ、「RAG（Retrieval Augmented Generation、検索拡張生成）」という言葉を耳にする機会が増えてきた。外部データベースの情報を検索して生成AIの出力に反映させ、回答の精度を高める技術だ。エクサウィザーズが2024年5月に302社／402人を対象として実施した調査では、約5割がRAGに取り組み中もしくは検討中、約4割が関心ありで、関心がないのは約1割に過ぎなかった。企業は生成AI活用の入り口としてRAGに挑戦し、チャットボットなどを導入しようとしている。もっとも、RAGの扱いは意外に難しい。

導入をあきらめる企業も多い
　「以前、社内の人事規定についての問い合わせに回答するRAGシステムを作成してPoC（概念実証）を実施したものの、回答精度が全く出なかった」。2023年度から生成AIチャットツールの東京ガスグループ内展開を進める東京ガスの笹谷俊徳DX推進部データ活用統括グループグループマネージャーはこのように振り返る。

　同社は生成AIを搭載した社内アプリケーション「AIGNIS（アイグニス）」を独自開発し、RAGを利用したチャットツール「AIGNIS-chat」を2024年10月に導入した。コールセンター対応や企画部門に寄せられる商材の問い合わせといったユースケースを想定する。笹谷グループマネージャーは、回答精度も含め「使える形にはできた」と自信を見せる。

　しかし、2023年度に生成AIチャットツールを利用し始めた当初は、RAGの回答精度は満足のいくものではなかった。そこで同社は、検索方法の見直しやデータ処理の改善といった地道な改良を重ね、回答精度を上げていった。これにより、RAGを実用レベルまで持っていくことに成功した。

　RAGはコンセプトが分かりやすいため、簡単に導入できると考える企業は多い。しかし、実際には実用的な精度を出すためのノウハウが必要になる。RAGを導入しようとしたが、十分な精度を出せずにあきらめてしまう企業は意外に多い。

追加学習のコストが不要
　RAGでは、大規模言語モデル（LLM）に問い合わせて回答を得る際に、ユーザーの質問に応じて外部データベースを参照し、検索結果とユーザーの質問を併せて送る。これにより、LLMがあらかじめ学習していない知識についても正確な情報を与えることができ、「ハルシネーション（幻覚）」を抑制する効果を期待できる。


　LLMに新しい情報を与える手法としては「ファインチューニング」もある。しかし、ファインチューニングはモデルの追加学習を行うため、そのためのコストがかかる。一方、RAGはモデルの学習は行わないため、トレーニングコストがかからない。こうした実装のハードルの低さが、多くの企業における導入につながっている。

　RAGでは、ユーザーが対話型AIのチャットなどに質問を入力すると質問のクエリーがいったんRAGのアプリケーションに送られる。アプリケーションはデータベースから質問に適した情報を抽出し、その結果と質問を含むプロンプトをLLMに送信。LLMはプロンプトを基に回答を生成する。

RAG（Retrieval-Augmented Generation）は、大規模言語モデル（LLM）に検索機能を組み合わせたAIを利用する新しい手法です。日本語では「検索拡張生成」などと訳されるこの技術により、生成AIによる回答の精度は高まり、さまざまな業務の効率化を促進できます。
通常、生成AIに新たな知識をインプットするには、専用の学習データを用意し、一定の時間をかけて追加学習を行う必要がありますが、RAGを使用すれば、追加学習のステップを省略できます。生成AIに質問をする際に、その質問に関連する情報を同時に提供することで、その情報に基づいた適切な回答を導き出すことができるのです。
たとえば「本日の社員食堂のおすすめを教えてほしい」と生成AIに聞いても、そもそも自社の社員食堂のメニューを知らない生成AIには回答できません。そこでRAGでは、生成AIに質問する際、同時に社員食堂のメニューのデータを渡します。こうすることで、生成AIが独自情報を踏まえた回答を行えるようになります。

生成AIにおけるRAG（検索拡張生成）とは？
仕組みや活用例をご紹介
生成AIにおけるRAG（検索拡張生成）とは？仕組みや活用例をご紹介
生成AIの利用において「自社のルールに沿った回答をさせたい」「自社の製品やサービスに関する知識を利用したい」といったニーズに対応できるのがRAGです。一般的に生成AIに知識をインプットするには追加学習が必要ですが、RAGを用いれば、追加学習を行わなくても生成AIに手軽に独自情報を読み込ませることができます。
この記事では、生成AIの活用の幅を広げてくれるRAGについて、その概要や必要性、仕組み、活用におけるポイントなどをご紹介します。

この記事の目次

RAGとは

なぜRAGが必要なのか

RAGの仕組み

RAGの活用例

RAG活用時のポイント

高精度のRAGを提供する「Alli LLM App Market」

まとめ

RAGとは
RAG（Retrieval-Augmented Generation）は、大規模言語モデル（LLM）に検索機能を組み合わせたAIを利用する新しい手法です。日本語では「検索拡張生成」などと訳されるこの技術により、生成AIによる回答の精度は高まり、さまざまな業務の効率化を促進できます。
通常、生成AIに新たな知識をインプットするには、専用の学習データを用意し、一定の時間をかけて追加学習を行う必要がありますが、RAGを使用すれば、追加学習のステップを省略できます。生成AIに質問をする際に、その質問に関連する情報を同時に提供することで、その情報に基づいた適切な回答を導き出すことができるのです。
たとえば「本日の社員食堂のおすすめを教えてほしい」と生成AIに聞いても、そもそも自社の社員食堂のメニューを知らない生成AIには回答できません。そこでRAGでは、生成AIに質問する際、同時に社員食堂のメニューのデータを渡します。こうすることで、生成AIが独自情報を踏まえた回答を行えるようになります。

なぜRAGが必要なのか
なぜRAGという仕組みが必要なのでしょうか。以下でその理由を解説します。

生成AI活用における課題
生成AIは、さまざまな用途に利用できる革新的な技術ではありますが、何でもできる魔法のツールではありません。用途のタイプに応じて向き不向きがあります。
たとえば、文章の作成や要約、翻訳などの言語処理や、プログラミングにおけるコードの自動生成、スプレッドシートで利用できる関数の作成など、「インターネット上でやり方を収集して学習できる一般的な作業」は、生成AIが得意とする領域です。
一方で、独自の規程や業務ルールなどに基づいた社内作業に関することは、そのままでは生成AIでの対応は困難です。当然ながら、生成AIは社内業務を進めるために必要となるルールなどを理解していません。社内作業について生成AIを活用するためには、何らかの形で社内情報を生成AIに理解させる必要があります。
皆さんが日常的に処理している業務の多くは、自社の規程やルールなどに基づいて行われるものではないでしょうか。経理や総務などのバックオフィス部門の業務や、営業・マーケティング部門における社内処理など、多くの業務は社内の業務ルールや業務プロセスに準ずる必要があります。このような業務にも生成AIを適用できれば、生成AIのユースケースが広がり、幅広い業務の効率化を実現できます。

RAGによる解決策
RAGは幅広い業務の効率化に有効です。RAGにより、社内情報などの独自情報を生成AIにインプットし、与えた情報を踏まえて回答を出力させます。これにより、たとえば社内規程に基づいた業務の進め方に関する質問など、これまで生成AIでは対応できなかったユースケースもカバーできるようになります。
ちなみに、利用時に社内情報を生成AIにインプットするのではなく、事前に生成AIに社内情報を追加学習させておくことでも、同様の処理が可能になります。この手法をファインチューニング（Fine-tuning）と呼びます。ただし、ファインチューニングを行うには、追加学習のために一定の時間やコストをかける必要があります。用途や目的によってはファインチューニングの方が適しているケースもありますが、手軽に独自情報を生成AIにインプットさせることができるのはRAGのメリットの1つです。

RAGの仕組み
RAGはどのような仕組みで動作するのでしょうか。RAGのプロセスの大まかな流れは以下のとおりです。

事前準備：「独自情報を検索できる検索用データベース」と「検索用データベースおよび生成AIと連携し、RAGの処理を実現するアプリケーション」を構築する

ユーザーはアプリケーションに教えてほしい内容を入力する
アプリケーションは検索用データベースから関連する独自情報を取得する
アプリケーションは独自情報とユーザーの質問内容を生成AIに入力し、生成AIは回答を作成する
アプリケーションはユーザーに回答結果を出力する
RAGの仕組みのイメージ
「検索用データベース」と「アプリケーション」の構築
事前準備として、自社の規程に関する資料や業務マニュアルなどの独自情報を格納した検索用データベースを構築し、ユーザーの質問内容に関連した独自情報を抽出するための検索機能を用意しておきます。
また、ユーザーからの質問を受け付け、検索用データベースと生成AIそれぞれと連携して、RAGの処理を実現するアプリケーションを用意します。

①ユーザーによる問い合わせ
ユーザーは事前に構築されたアプリケーションに対して、質問内容（プロンプト）を入力します。たとえば「500万円の発注を行いたいが、誰の決裁を取得すればよいのか」といった質問内容が考えられます。

②関連する独自情報の取得
アプリケーションは、入力された質問内容に基づき検索用データベースを検索して、関連する独自情報を取得します。先ほどの質問では、社内の権限規程などが回答に必要な情報として該当します。

③生成AIによる回答作成
アプリケーションは、ユーザーが入力した質問内容と検索用データベースから取得した独自情報をセットにして、生成AIに入力します。生成AIは、事前に学習済みの一般的な情報といま入力された独自情報を踏まえたうえで、ユーザーの質問に対し適切な回答を出力します。
先ほどの例では、「500万円の発注を行う場合、部長決裁を取得する必要があります」といった回答が生成AIから得られます。

④ユーザーへの出力
最終的に、アプリケーションは生成AIの回答結果をユーザーに対して表示します。以上が、RAGを用いた生成AI活用の一連のプロセスです。

RAGの活用例のイメージ
RAGの活用例
では、RAGは具体的にどのような場面で利用できるのでしょうか。主な活用例をご紹介します。

社内ルールを検索するAIチャットボットサービス
RAGを利用して、社内規程や業務マニュアルなどを手軽に調べられる社内向けサービスを構築できます。一般的に、企業規模が拡大するほど規程や業務マニュアルなども増える傾向があり、大企業では目的のドキュメントを探すだけでもかなりの時間がかかるでしょう。特に新入社員や転職してきたばかりの従業員にとっては、欲しい情報がなかなか見つからず苦労する場面が多いのではないでしょうか。
RAGを活用したAIチャットボットサービスにより、手軽に社内情報を検索する仕組みを構築して、こうした課題を解決できます。

カスタマーサポートでの顧客対応
RAGを用いて自社の製品やサービスに関する情報をインプットすることで、カスタマーサポートでの顧客対応に生成AIを活用できます。
近年、コールセンターやコンタクトセンターでは人手不足が重大な課題となっています。そこで、定型的なサポート内容についてはRAGを用いた生成AIに一任し、AIでは処理しきれないイレギュラーな対応などに限定してオペレーターがサポートを行うように設計すれば、入電数を削減できます。RAGの活用は、コールセンター・コンタクトセンターの人手不足問題の解決につながります。

ヘルプデスクサービス
社内システムのヘルプデスクサービスを提供している場合、RAGを用いてサービスの一部を代行できます。RAGによりシステムのマニュアルを生成AIに読み込ませることで、ユーザーからのシステム操作に関する質問への対応や、トラブルシューティングなどの一部がAIで可能になります。これにより、ヘルプデスクの業務負荷を軽減できるでしょう。

RAG活用時のポイント
RAGの活用においては、セキュリティ面、精度面などいくつか注意すべき点もあります。以下では、そうしたポイントをご紹介します。

セキュリティの担保
RAGでは、質問内容と独自情報を合わせて生成AIに入力しますが、生成AIに読み込ませる独自情報の多くは、社外秘、部外秘などに指定される機密性の高いものです。これらの情報が流出しないよう、RAGの利用にあたってはセキュリティに最大限の注意を払わなければなりません。
利用する生成AIや選択するプランによっては、入力された情報を生成AIが再学習に利用すると規定されたものもあります。このような規約を掲げたRAGを使用してしまうと、独自の機密情報が生成AIの再学習に利用され、ほかの利用者の質問に対する回答として出力され外部に漏洩してしまう可能性が生じます。こうした事態を防ぐため、利用規約を十分に確認・理解したうえで生成AIのサービスやプランを選択する必要があります。

出力精度
RAGは生成AIに独自情報を手軽に与えることのできる手法ではありますが、単に社内規程などのドキュメントを読み込ませるだけでは、十分な精度が出ない場合もあります。
高精度の回答生成を求める場合、ユーザーが入力した内容を踏まえてデータベースを検索し、いかに関連性の高い情報を特定できるかが重要です。これには検索用データベースの高い検索精度が不可欠です。メタデータの活用やセマンティック検索の活用など、検索精度を高めるための手法はさまざまです。しかし、RAGを効果的に利用するには、まずはデータベースの検索精度が重要である点を理解しましょう。

高精度のRAGを提供する「Alli LLM App Market」
「Alli LLM App Market」は、生成AIを企業で簡単に利用するための環境をオールインワンで提供します。Alli LLM App Marketでは、生成AIをさまざまな業務で活用できるメニューを、生成AIから有効な回答を得るためのノウハウが詰まったプロンプトとともに提供しています。また独自のRAGシステムにより、表形式を含むドキュメントも高精度で検索して生成AIにインプットできたり、RAGによる回答結果に対してフィードバックを与えて段階的に精度を高めたりできるので、上述したRAG利用時の注意点である「出力精度が上がりにくい」といった問題にも対処できます。

日立ソリューションズは、Alli LLM App Market の開発元であるAllganize Japan株式会社と販売代理店契約を結んでおり、Alli LLM App Marketの再販および提案・導入作業を実施しています。日立ソリューションズの長年にわたるシステムインテグレーションの実績により、生成AI・LLMで機密情報を安全に取り扱えるプライベート環境を提供できます。また、既存システムとの連携など、お客さまの要望に合わせて適切な生成AI・LLM活用環境を提供します。

まとめ
この記事では、RAGの仕組みや具体的なユースケース、利用時の注意点などについてご紹介しました。生成AIの活用の幅を広げられるRAGは、うまく使いこなせば業務効率化や生産性向上のための大きな武器となります。一方で、RAGの環境を構築するためには一定の知見が必要なのも事実です。Alli LLM App Marketでは、企業向けの生成AI利用環境をオールインワンで提供しており、高精度のRAGも利用できます。ご興味のある方は、ぜひ当社までお問い合わせください。

検索拡張生成（RAG）とは
RAG（検索拡張生成）は、従来の情報検索システム（検索やデータベースなど）の長所と、生成大規模言語モデル（LLM）の機能を組み合わせた AI フレームワークです。独自のデータと世界に関する知識を LLM の言語スキルと組み合わせることで、根拠のある生成はより精度が高く最新の、特定のニーズに関連したものとなります。この電子書籍をご覧になり、企業の実体を明らかにしましょう。

RAG とは何かを表す画像
35:30
Vertex AI Search と DIY RAG による Gemini の根拠づけ
検索拡張生成の仕組み
RAG は、生成 AI の出力を強化するために、いくつかの主要な手順で動作します。

取得と前処理: RAG は、強力な検索アルゴリズムを活用して、ウェブページ、ナレッジベース、データベースなどの外部データをクエリします。関連情報が取得されると、トークン化、ステミング、ストップワードの削除などの前処理が行われます。
根拠に基づく生成: 取得された情報の前処理が終わると、事前トレーニング済みの LLM にシームレスに組み込まれます。この統合により、LLM のコンテキストが強化され、トピックをより包括的に理解できるようになります。この補強されたコンテキストにより、LLM はより正確で有益な、魅力的な回答を生成できるようになります。
RAG を使用する理由
RAG には、従来のテキスト生成方法を補完するいくつかの利点があります。特に、事実情報やデータドリブンな回答を扱う場合にその利点が生かされます。RAG を使用するとメリットがあることには、次のような主な理由があります。

新しい情報へのアクセス
LLM は、事前トレーニング済みデータに限定されます。そのため、情報が古く、不正確であることが考えられるレスポンスを生成する場合があります。RAG は、LLM に最新の情報を提供することによってこれを克服します。

事実に基づく根拠付け
LLM は、創造的で魅力的なテキストを生成するための優れたツールですが、事実の正確性については苦慮することになる場合があります。これは、LLM が大量のテキストデータでトレーニングされていて、そのテキストデータに不正確さやバイアスが含まれている可能性があるためです。

入力プロンプトの一部として LLM に「事実」を提示することで、「生成 AI のハルシネーション」を軽減できます。このアプローチの要点は、ユーザーの質問に答え、システムの指示と安全上の制約を遵守しつつ、最も関連性の高い事実が LLM に提示され、LLM の出力が完全にそれらの事実に基づいている状態を確保することです。

Gemini の長いコンテキスト ウィンドウ（LCW）を使用すると、LLM にソース マテリアルを提供できます。LCW に収まる量を超える情報を提供する必要がある場合や、パフォーマンスをスケールアップする必要がある場合は、トークンの数を減らし、時間と費用を節約できる RAG アプローチを使用します。

ベクトル データベースと関連性についての re-ranker を使用して検索する
RAG は通常、検索によって事実を取得します。最新の検索エンジンは、ベクトル データベースを活用して関連ドキュメントを効率的に取得するようになりました。ベクトル データベースは、ドキュメントをエンベディングとして高次元空間に保存し、セマンティックな類似性に基づいて高速かつ正確に検索できるようにします。マルチモーダル エンベディングは画像、音声、動画などに使用でき、これらのメディア エンベディングはテキスト エンベディングやマルチランゲージ エンベディングとともに取得できます。

Vertex AI Search などの高度な検索エンジンでは、セマンティック検索とキーワード検索を組み合わせて使用します（ハイブリッド検索と呼ばれます）。また、検索結果にスコアを付け、上位に返される結果が最も関連性が高い状態にする re-ranker も使用します。また、検索結果は、誤字のない明確で的を絞ったクエリで検索した方が向上します。そのため、高度な検索エンジンは、検索前にクエリを変換してスペルミスを修正します。

関連性、精度、品質
RAG の検索メカニズムは非常に重要です。キュレートされたナレッジベースの上に最適なセマンティック検索を構築して、取得された情報が入力クエリまたはコンテキストに関連している状態にする必要があります。取得した情報が無関係である場合、生成された情報は根拠があるものの、話題から外れているか、間違っている可能性があります。

RAG は、LLM をファインチューニングまたはプロンプト エンジニアリングして、すべて取得した知識に基づいてテキストを生成することで、生成されたテキストの矛盾や不整合を最小限に抑えます。これにより、生成されるテキストの品質が大幅に向上し、ユーザー エクスペリエンスが向上します。

Vertex Eval Service は、LLM で生成されたテキストと取得されたチャンクを「coherence」、「fluency」、「groundedness」、「safety」、「instruction_following」、「question_answering_quality」などの指標に基づいてスコア付けするようになりました。これらの指標によって、LLM から得られる根拠のあるテキストを測定できます（一部の指標については、提示したグラウンド トゥルースの回答と比較されます）。これらの評価を実施することで、ベースラインの測定値が得られ、検索エンジンの構成、ソースデータのキュレート、ソースレイアウト解析やチャンキング戦略の改善、検索前のユーザーの質問の精査などによって、RAG の品質を最適化できます。このような RAG の運用、指標重視のアプローチは、高品質な RAG と根拠のある生成を実現するための取り組みに活用できます。

RAG、エージェント、chatbot
RAG とグラウンディングは、最新のデータ、プライベートなデータ、または特殊なデータへのアクセスを必要とする LLM アプリケーションやエージェントに統合できます。外部の情報にアクセスすることで、RAG を利用した chatbot と会話エージェントが外部の知識を活用し、より包括的で情報に富み、コンテキストアウェアな回答を提供できるようにし、全体的なユーザー エクスペリエンスを向上させます。

生成 AI で構築する内容を差別化する要因は、データとユースケースです。RAG とグラウンディングにより、データを LLM に効率的かつスケーラブルに適用できます。

Retrieval-Augmented Generation は、外部ソースから取得した情報を用いて、生成 AI モデルの精度と信頼性を向上させるテクノロジです。

生成 AI の最新の進歩を理解するために、法廷を想像してみてください。

裁判官は、一般的な法律の理解に基づいて審理し、判決を下します。時には、医療ミス訴訟や労働争議など、特定の専門知識が必要なケースもあるため、裁判官は裁判所書記官を法務図書館に送り、引用できる判例や具体的な事例を探させます。

優れた裁判官のように、大規模言語モデル (LLM) は人間の様々なクエリに答えることができます。しかし、出典を引用した信頼できる回答を提供するためには、モデルにも調査を行うアシスタントが必要です。

AI の裁判所書記官は、Retrieval-Augmented Generation 、略して RAG と呼ばれるプロセスです。

名前の物語
この名前を生み出した 2020 年の論文の主執筆者である Patrick Lewis 氏 は、現在では生成 AI の未来を示す何百もの論文や何十もの商用サービスにわたって増え続けている手法を説明するための、このお世辞にも良いとは言えない略語について謝罪しました。

Lewis 氏は、「私たちの仕事がこれほど広まるとわかっていたら、間違いなくこの名前にもっと工夫を凝らしたでしょう」と、データベース開発者の地域会議で自身のアイデアを披露していたシンガポールでのインタビューで語りました。


Patrick Lewis 氏
現在 AI スタートアップの Cohere で RAG チームを率いる Lewis 氏は次のようにも述べています。「私たちは、もっと響きの良い名前にするつもりでしたが、いざ論文を書こうとすると、誰もより優れたアイデアを思いつきませんでした」

Retrieval-Augmented Generationとは何か？
Retrieval-Augmented Generation は、外部ソースから取得した情報を用いて、生成 AI モデルの精度と信頼性を向上させるテクノロジです。

いわば、LLM の機能の不足を補うものです。LLM はその内部ではニューラルネットワークであり、一般的にはパラメーター数で評価されます。LLM のパラメーターは基本的に、人間がどのように単語を使って文章を作るかという一般的なパターンを表しています。

パラメーター化された知識と呼ばれることもあるこの深い理解によって、LLM は一般的なプロンプトに光速で応答するのに役立ちます。しかし、最近のトピックやより具体的なトピックに深く入り込みたいユーザーには役に立ちません。

内外のリソースを組み合わせる
Lewis 氏 たちは、生成 AI サービスを外部リソース、特に最新の技術的詳細に富むリソースにリンクさせるために、Retrieval-Augmented Generation を開発しました。

旧 Facebook AI Research (現 Meta AI)、ユニバーシティ カレッジ ロンドン、ニューヨーク大学の共著によるこの論文は、RAG を「汎用的なファインチューニング レシピ」と呼んでいます。

ユーザーの信頼を築く
Retrieval-Augmented Generation は、研究論文の脚注のように引用できるソースをモデルに与えます。これにより、ユーザーはあらゆる回答をチェックすることができ、それが信頼に繋がります。

さらに、このテクノロジはユーザーのクエリのあいまいさを解消するのに役立ちます。また、幻覚と呼ばれることもある、モデルが間違った推測をする可能性も減らすことができます。

RAG のもう一つの大きな利点は、比較的簡単だということです。Lewis 氏と論文の共著者 3 人によるブログによれば、開発者はわずか 5 行のコードでこのプロセスを実装できるといいます。

そのため、この手法は追加のデータセットでモデルを再トレーニングするよりも高速かつ低コストです。また、新しいソースをその場でホットスワップすることもできます。

Retrieval-Augmented Generation をどう使うか
Retrieval-Augmented Generation を使えば、ユーザーは基本的にデータ リポジトリと会話をすることができ、新しい体験を可能にします。つまり、RAG のアプリケーションは、何倍ものデータセットを利用できる可能性があるということです。

例えば、医療指標で補完された生成 AI モデルは、医師や看護師にとって素晴らしいアシスタントになるでしょうし、金融アナリストは、市場データとリンクしたアシスタントから恩恵を受けるでしょう。

実際、ほとんどすべてのビジネスで、技術マニュアルやポリシー マニュアル、ビデオやログをナレッジ ベースと呼ばれるリソースに変えることができ、LLM を強化できます。これらのソースは、顧客や現場のサポート、従業員のトレーニング、開発者の生産性向上といったユースケースを可能にします。

AWS、IBM、Glean、Google、Microsoft、NVIDIA、Oracle、Pinecone などの企業が RAG を採用している理由は、その幅広い可能性にあります。

Retrieval-Augmented Generation を始めよう
NVIDIA は、ユーザーが始めやすいように、Retrieval-Augmented Generation のリファレンス アーキテクチャを開発しました。このリファレンス アーキテクチャには、チャットボットのサンプルや、ユーザーがこの新しいメソッドによる独自のアプリケーションを作成するために必要な要素が含まれています。

このワークフローでは、生成 AI モデルの開発とカスタマイズのためのフレームワークである NVIDIA NeMo や、生成 AI モデルを実運用で実行するための NVIDIA Triton Inference Server や NVIDIA TensorRT-LLM などのソフトウェアを使用します。

このソフトウェア コンポーネントはすべて、NVIDIA AI Enterprise の一部であり、企業が必要とするセキュリティ、サポート、安定性を備えた、実運用可能な AI の開発と展開を加速するソフトウェアプ ラットフォームです。

RAG ワークフローで最高のパフォーマンスを得るには、データを移動し処理するための大量のメモリとコンピューティングが必要です。288GB の高速な HBM3e メモリと 8 ペタフロップスの演算能力を持つ NVIDIA GH200 Grace Hopper Superchip は理想的であり、CPU を使用する場合に比べて 150 倍の高速化を実現できます。

企業が RAG に慣れれば、既製またはカスタムの様々な LLM を内部または外部のナレッジベースと組み合わせて、従業員や顧客を支援する様々なアシスタントを作成することができます。

RAG はデータセンターを必要としません。NVIDIA ソフトウェアが可能にするあらゆる種類のアプリケーションは、ユーザーがノート PC でもアクセスできるので、LLM を Windows PC で動かすこともできます。


PC 上での RAG のアプリケーション例
NVIDIA RTX GPU を搭載した PC は、一部の AI モデルをローカルで実行することができます。PC 上で RAG を使用することで、ユーザーは電子メール、メモ、記事など、プライベートなナレッジソースにリンクし、回答を改善させることができます。そしてユーザーは、データソース、プロンプト、回答がすべてプライベートで安全であると確信できます。

最近のブログでは、TensorRT-LLM for Windows によって RAG を高速化し、より良い結果を迅速に得る例を紹介しています。

Retrieval-Augmented Generation の歴史
このテクノロジのルーツは、少なくとも 1970 年代初頭にさかのぼります。その頃、情報検索の研究者たちは、自然言語処理 (NLP) を使ってテキストにアクセスする質問応答システムと呼ばれるアプリで試作し、それは最初、野球のような狭いトピックのものでした。

この種のテキスト マイニングの背後にある概念は、長年にわたってかなり不変です。しかし、それを駆動する機械学習エンジンは大きく成長し、その有用性と人気を高めています。

1990 年代半ば、Ask Jeeves サービス (現在の Ask.com) は、身なりの良い付き人のマスコットで質問応答を普及させました。IBM の Watson は 2011 年、クイズ番組『Jeopardy !』で 2 人の人間のチャンピオンを見事に破り、一躍テレビ界の有名人になりました。


今日、LLM は質問応答システムをまったく新しいレベルに引き上げています。

ロンドンの研究室からの洞察
Lewis 氏がユニバーシティ カレッジ ロンドンで NLP の博士号を取得し、Meta 社の新しいロンドンの AI ラボで働いていたとき、2020 年の独創的な論文が公開されました。チームは、LLM のパラメータに多くの知識を詰め込む方法を模索し、その進捗状況を測定するために開発したベンチマークを使用していました。

以前の方法をベースに、Google の研究者らが発表した論文に触発されたこのグループは、「中央に検索インデックスがあり、学習して任意のテキスト出力を生成できるトレーニングされたシステムという、説得力のあるビジョンを持っていました」と Lewis 氏は振り返ります。


IBM の質問回答システム「Watson」は、テレビのクイズ番組『Jeopardy!』で大勝し、一躍有名人となりました
Lewis 氏が、Meta の別のチームが開発した有望な検索システムを、現在進行中の研究に取り入れたところ、最初の結果は予想外に素晴らしいものでした。

「上司に見せたら、『おお、これは大成功だ。このようなことは滅多に起こらない』と言いました。こういったワークフローは最初に正しく設定するのが難しいからです」

Lewis 氏 はまた、チーム メンバーの Ethan Perez 氏と Douwe Kiela 氏 (それぞれニューヨーク大学と Facebook AI Research 出身) の大きな貢献も認めています。

NVIDIA GPU のクラスタ上で実行されたこの研究は、完成後、生成 AI モデルをより権威のある信頼できるものにする方法を示しました。それ以来、この研究は何百もの論文に引用され、現在も活発な研究分野として、その概念を増幅、拡張しています。

Retrieval-Augmented Generation の仕組み
NVIDIA のテクニカル ブリーフに記載されている RAG プロセスの概要は以下の通りです。

ユーザーが LLM に質問すると、AI モデルはクエリを別のモデルに送り、機械が読めるように数値フォーマットに変換します。クエリの数値バージョンは、エンベッディングやベクトルと呼ばれることもあります。


LLM とエンベッディング モデルとベクトル データベースを組み合わせた Retrieval-augmented Generation
エンベッディング モデルは、これらの数値を、利用可能なナレッジ ベースの機械可読のインデックス内のベクトルと比較します。一致する、あるいは複数の一致が見つかると、関連するデータを検索し、人間が読める単語に変換して LLM に戻します。

最後に、LLM は、検索された単語とクエリに対する独自の応答を組み合わせて、最終的な回答を作成し、エンベッディング モデルが見つけたソースがあれば引用してユーザーに提示します。

ソースを最新に保つ
バックグラウンドでは、エンベッディング モデルは、新しい知識ベースや更新された知識ベースが利用可能になると、機械可読のインデックス (ベクトル データベースと呼ばれることもある) を継続的に作成、更新します。


LangChain のダイアグラムは、検索プロセスを使った LLM の別の姿を示しています
多くの開発者は、オープンソース ライブラリである LangChain が、LLM、エンベッディング モデル、ナレッジ ベースを連結する際に特に有用であると感じています。NVIDIA は LangChain を Retrieval-Augmented Generation のリファレンス アーキテクチャに使用しています。

LangChain コミュニティは、RAG プロセスについて独自の説明を提供しています。

今後、生成 AI の未来は、あらゆる種類の LLM とナレッジ ベースを創造的に連鎖させ、ユーザーが検証できる権威ある結果を提供する新しい種類のアシスタントを生み出すことにあります。

この NVIDIA LaunchPad ラボで、AI チャットボットによる Retrieval-Augmented Generation を実際に使ってみましょう。
